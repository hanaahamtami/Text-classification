{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Convolutional Neural Network CNN for Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Movie Review Data est une collection de critiques de films extraites du site Web imdb.com au début des années 2000 par Bo Pang et Lillian Lee. Les revues ont été rassemblées et mises à disposition dans le cadre de leurs recherches sur le traitement du langage naturel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composer de: \n",
    "- 1 000 critiques de films positives \n",
    "- 1 000 critiques de films négatives \n",
    "\n",
    "tirées des archives du groupe de discussion rec.arts.movies.reviews hébergé sur imdb.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def process_docs(directory, vocab,is_trian):\n",
    "    documents = list()\n",
    "    for filename in listdir(directory):\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_list(tokens, 'vocab.txt')\n",
    "negat_docs = process_docs('txt_sentoken/neg', vocab,True)\n",
    "posit_docs = process_docs('txt_sentoken/pos', vocab,True)\n",
    "train_docs = negat_docs + posit_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "##code les documents d'apprentissage sous forme de  séquences d'entiers \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25768\n"
     ]
    }
   ],
   "source": [
    "#taille du vocabulaire\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Une configuration CNN avec 32 filtres  \n",
    "#une taille de noyau de 8 \n",
    "# une fonction d'activation linéaire rectifiée («relu»)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la couche d’incorporation attend des documents d’une longueur de 1373 mots et encode chaque mot du document en tant que vecteur de 100 éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons une fonction de perte d'entropie croisée binaire car le problème que nous apprenons est un problème de classification binaire. La mise en œuvre efficace par Adam de la descente de gradient stochastique est utilisée et nous gardons une trace de la précision en plus de la perte pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanaa\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 11s - loss: 0.6899 - accuracy: 0.5478\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.5147 - accuracy: 0.7678\n",
      "Epoch 3/10\n",
      " - 11s - loss: 0.0756 - accuracy: 0.9922\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 10s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 11s - loss: 7.5061e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 10s - loss: 4.7058e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 10s - loss: 3.1726e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15feb6b6f48>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compile réseau\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.000001\n"
     ]
    }
   ],
   "source": [
    "# evaluer\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que le modèle atteint très rapidement 100% de précision sur l'ensemble de données d'apprentissage.\n",
    "\n",
    "**À la fin de l'analyse, le modèle atteint une précision de 86% sur l'ensemble de données de test, ce qui constitue un excellent score.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
